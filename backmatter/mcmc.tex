% !TEX encoding = UTF-8 Unicode
% !TEX TS-program = LuaLaTeX
% !TEX root = ../memoire.tex
% !TEX spellcheck = fr-FR

%*******************************************************
% Declaration
%*******************************************************
\cleardoublepage
\phantomsection
\chapter{Markov Chain Monte-Carlo}\label{annexe:mcmc}

\section{Statistiques Bayesiennes et Chaines de Markov}

\begin{remark}{Pour approfondir}
    Pour une introduction rapide aux techniques MCMC il est possible de voir \citet{Goncalves2015}, \citet{Lam2008} où \citet{Andrieu2003}. Pour une véritable justification, voir \citet{Robert2004b}.
\end{remark}

Nous avons vu que toutes les méthodes bayésiennes possédaient le même fonctionnement de base: la spécification d'un a priori $\mathbb{P} ( \theta )$ sur les paramètres d'intérêt, puis la mise à jour de cet a priori en fonction des observations en utilisant la loi de Bayes afin d'obtenir un a posteriori $\mathbb{P} ( \theta \mid \mathbf{x} )$.
\begin{equation*}
    \mathbb{P} ( \theta ) \varpropto \mathbb{P} ( \mathbf{x} \mid \theta ) \mathbb{P} ( \theta )
\end{equation*}
Enfin, puisque souvent la valeur qui nous intéresse est $f(\theta)$, on obtient l'estimateur de Bayes associé:
\begin{equation}
    \mathbb{E} \left[ f(\theta) \mid \mathbf{x} \right]\label{equ:integrale_bayes}
\end{equation}
Cet estimateur est intéressant puisqu’admissible, convergeant en probabilité et asymptotiquement normale.

La procédure standard serait alors de chercher à estimer~\ref{equ:integrale_bayes} par intégration de Monte-Carlo:
\begin{align*}
    &\mathbb{E} \left[ f(\theta) \mid \mathbf{x} \right] = \lim_{n \to \infty} \frac{1}{n} \sum_{i = 1}^n f(\tilde{\theta}_i) \\
    &\tilde{\theta}_i \sim \theta \mid \mathbf{x} \\
    &\tilde{\theta}_i \text{ indépendants} \\
    &\mathbb{E} \left[ \vert \tilde{\theta}_1 \vert \right] < +\infty
\end{align*}

Il existe néanmoins un obstacle de taille à la mise en place de ce calcul puisque le calcul de $\mathbb{E} \left[ f(\theta) \mid \mathbf{x} \right]$ suppose que l'on sache simuler des réalisations de $\mathbb{P} ( \theta \mid \mathbf{x} )$ indépendantes. La simulation d'une telle loi est dans certains cas possibles:
\begin{itemize}
    \item la loi est une loi déjà connue et simulable directement.
    \item la loi est associée à une fonction de répartition inversible ou pseudo-inversible.
    \item on connaît une autre loi qui borne celle-ci et permet donc de procéder à une procédure d'acceptation-rejet.
\end{itemize}

Pour les lois complexes, ce sera presque toujours le cas, il est impossible d'avoir recours à de telles méthodes. Obtenir des réalisations indépendantes des $\tilde{\theta}_i$ est trop contraignant et nous pourrons au mieux obtenir des réalisations corrélées de $\theta \mid \mathbf{x}$. Il semble alors nécessaire de posséder une généralisation de la \ac{lfgn} dans le cas où les observations ne sont pas i.i.d.

\begin{theoreme}[Théorème Ergodique]\label{thrm:ergodique}
    Soient $\hat{\theta}_1,\dotsc,\hat{\theta}_m$ une suite de valeurs d'une chaîne de Markov de noyaux de transition $g$, c'est à dire 
    \begin{equation*}
        \mathbb{P} \left( \hat{\theta}_i \mid \hat{\theta}_{i-1},\dotsc,\hat{\theta}_1 \right) = g ( \hat{\theta}_{i-1} , \hat{\theta}_i )
    \end{equation*}
    et matrice de transition $P$, telle que la chaîne soit:
    \begin{description}
        \item[apériodique] \begin{equation*}
                \exists i , \forall j > i,  g \left( \hat{\theta}_j \mid \hat{\theta}_i \right) > 0
            \end{equation*}
        \item[irréductible] \begin{equation*}
                \forall i,j \; g \left( \hat{\theta}_j \mid \hat{\theta}_j \right) > 0
            \end{equation*}
        \item[positive récurrente] \begin{align*}
            &\forall i , \mathbb{E} \left[ T_i \right] < \infty \\
            &T_i = \inf \left\{ j \geq 1 : \hat{\theta}_j = i \mid \hat{\theta}_0 = i \right\}
        \end{align*}
    \end{description}
    Alors si $\mathbb{E} \left[ f(\theta) \right] < \infty$ alors
    \begin{equation*}
        \mathbb{P} \left( \frac{1}{n} \sum_{i=1}^n f(\hat{\theta}_i) \to \int f(\theta) \pi ( \theta ) \diff \theta \right)
    \end{equation*}
    où $\pi$ est la distribution stationnaire de la chaîne de Markov c'est-à-dire $\pi = \pi P$
\end{theoreme}

Nous allons donc chercher à créer une chaîne de Markov répondant aux hypothèses de~\ref{thrm:ergodique} de distribution stationnaire égale à $\mathbb{P} (\theta \mid \mathbf{x})$.

\section{Échantillonage de Metropolis-Hastings}\label{sec:metropolis.hastings}

Il semble naturel de se poser la question \textquote{Est-il toujours possible de construire une chaîne de Markov répondant aux hypothèses du théorème~\ref{thrm:ergodique}?}. La réponse est oui.
En effet une condition suffisante (mais non nécessaire) pour assurer que $p(\theta)$ est la distribution invariante de notre chaîne est la condition de réversibilité suivante, aussi appelée équilibre détaillé:
\begin{equation}\label{equ:detailed.balance}
    p( \theta_i ) g ( \theta_{i-1} \mid \theta_i ) = p( \theta_{i-1} ) g ( \theta_{i} \mid \theta_{i-1} )
\end{equation}

L'algorithme de Metropolis-Hastings utilise alors une densité de proposition $q (\theta' \mid \theta)$, qui joue le rôle de chaîne de Markov continue. À partir de $\theta$, une valeur $\theta'$ est proposée puis acceptée comme nouvel état avec probabilité:
\begin{equation*}
    A(\theta,\theta') = \min \left\{ 1 , \frac{p(\theta') q(\theta \mid \theta')}{p(\theta) q(\theta'\mid \theta} \right\}
\end{equation*}

L'algorithme de Metropolis-Hastings est alors:

\begin{algorithm}
\caption{Metropolis-Hastings} \label{metropolis.hastings.alg}
\begin{algorithmic}
    \Procedure{Metropolis-Hastings}{$q, \theta_0, n$}
    \For{$i \in [0,n-1] $}
        \State{Tire $u \sim \mathscr{U}_{[0,1]} $}
        \State{Tire $\theta' \sim q(\theta' \mid \theta_i)$}
        \If{$u < \min \left\{ 1 , \frac{p(\theta') q(\theta_i \mid \theta')}{p(\theta_i) q(\theta'\mid \theta_i)} \right\}$}
            \State{$\theta_{i+1} = \theta'$}
        \Else \State{$\theta_{i+1} = \theta_i$}
        \EndIf
    \EndFor
    \State \Return $(\theta_0,\cdots,\theta_{n-1})$
    \EndProcedure
\end{algorithmic}
\end{algorithm}

On a ici que
\begin{equation*}
    g( \theta_i , \theta_{i+1} ) = q ( \theta_{i+1} \mid \theta_{i} ) A(\theta_{i},\theta_{i+1}) + \delta_{\theta_{i}} ( \theta_{i+1} ) R(\theta_{i})
\end{equation*}
Avec $R\theta_{i}$ le terme de rejet:
\begin{equation*}
    R(\theta_{i}) = \int q ( \theta \mid \theta_{i} ) ( 1 - A(\theta_{i},\theta) ) \diff \theta
\end{equation*}
Par construction $g$ remplis la propriété~\ref{equ:detailed.balance}. De plus, le terme de rejet garantit l'apériodicité et il suffit que le support de $q$ inclue le support de $p$ pour garantir l'irréductibilité.

La présence de $p$ non seulement au numérateur, mais aussi au dénominateur permet de se débarrasser de la constante de renormalisation, souvent l'un des éléments les plus contraignants.

\section{Échantillonnage de Gibbs}\label{gibbs}

Il existe un cas dans lequel l'algorithme de Metropolis-Hastings possède une forme particulière: le cas où l'on connaît les lois conditionnelles complètes de tous les paramètres, c'est-à-dire si l'on sait tirer une valeur de $\mathbb{P} (\theta_i \mid \theta_{-i} , \mathbf{x})$, $\forall i$.

Dans ce cas il est possible de poser:
\begin{equation*}
    q ( \theta' \mid \theta_i ) = \begin{cases}
        p( \theta^{'(j)} \mid \theta_{i}^{(-j)} ) \text{ si } \theta^{'(-j)} = \theta_i^{(-j)} \\
        0 \text{ sinon.}
    \end{cases}
\end{equation*}
Et la probabilité d'acceptation est alors
\begin{align*}
    A( \theta_i , \theta' ) &= \min \left\{ 1 , \frac{p(\theta') q ( \theta_i \mid \theta' )}{p(\theta_i) q ( \theta' \mid \theta_i )} \right\} \\
    &= \min \left\{ 1 , \frac{p(\theta') p(\theta_{i}^{(j)} \mid \theta^{'(-j)})}{p(\theta_i) p( \theta^{'(j)} \mid \theta_{i}^{(-j)} )} \right\} \\
    &= \min \left\{ 1 , \frac{p(\theta^{(-j)}) p(\theta') \overbrace{p(\theta_{i}^{(j)} \mid \theta^{'(-j)}) p ( \theta^{'(-j)} )}^{ p( \theta_i ) } }{ p(\theta^{'(-j)}) p(\theta_i) \underbrace{p( \theta^{'(j)} \mid \theta_{i}^{(-j)} ) p(\theta^{(-j)})}_{ p(\theta' )}} \right\} \\
    &= \min \left\{ 1 , \frac{p(\theta^{(-j)}) }{ p(\theta^{'(-j)}) } \right\} \\
    \text{(car } \theta^{'(-j)} = \theta_i^{(-j)} \text{)} \qquad  &= 1
\end{align*}

L'algorithme d'échantillonnage de Gibbs est, en notant $\theta = ( \theta_1 , \dotsc , \theta_p )$, le vecteur des paramètres et $\theta^i$ la $i^\text{ème}$ itération.
\begin{itemize}
    \item Choix de valeurs initiales pour $\theta^0$.
    \item Tirage de $\theta^1_1 \sim \theta_1 \mid \theta^0_{-1}$
    \item $\theta^0$ est mis à jour à l'aide du tirage précédent.
    \item Les $\theta^1_i$ sont tirés de la même façon jusqu'a ce qu'ils aient tous été mis à jour afin d'obtenir $\theta^1$.
    \item La procédure est répétée $M$ fois.
\end{itemize}

La suite $\left( \theta^i \right)$ qui en résulte répond aux propriétés de Markov souhaitées.


